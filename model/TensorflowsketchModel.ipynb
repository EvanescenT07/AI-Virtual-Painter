{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnwV8cFwlpaW",
        "outputId": "2b394b23-d21d-421b-f8ca-9557dded06ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZRXPFWHmcNS"
      },
      "source": [
        "# **Download Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T39Jq3wjmbDy"
      },
      "outputs": [],
      "source": [
        "!mkdir Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "yPysDJ7emopB",
        "outputId": "491abb36-8269-47b4-c8d4-c78b874f8370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading airplane...\n",
            "airplane downloaded.\n",
            "Downloading banana...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0ef419bea314>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{c} downloaded.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdownload_all_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-0ef419bea314>\u001b[0m in \u001b[0;36mdownload_all_classes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Downloading {c}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dataset/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{c} downloaded.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "def download_all_classes():\n",
        "    # List of all class names in the Quick, Draw! dataset\n",
        "    all_classes = [\n",
        "        'airplane','banana', 'basketball', 'book',\n",
        "        'car', 'cat', 'fish', 'flower', 'guitar',\n",
        "        'house', 'laptop', 'lion', 'moon', 'mountain', 'pencil',\n",
        "        'star', 'tree', 'umbrella','sun', 'cloud', 'house', 'lightning'\n",
        "    ]\n",
        "\n",
        "    base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "\n",
        "    for c in all_classes:\n",
        "        cls_url = c.replace('_', '%20')\n",
        "        path = base + cls_url + '.npy'\n",
        "        print(f'Downloading {c}...')\n",
        "        urllib.request.urlretrieve(path, 'Dataset/' + c + '.npy')\n",
        "        print(f'{c} downloaded.')\n",
        "\n",
        "download_all_classes()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8l6zFyKoZFN"
      },
      "source": [
        "# **Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfPX4bE6oYxK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G24e5Fqooqim"
      },
      "source": [
        "# **Load and preprocess the dataset using generators**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwEz4FaVopxs"
      },
      "outputs": [],
      "source": [
        "def data_generator(data_dir, batch_size):\n",
        "    class_names = [os.path.splitext(f)[0] for f in os.listdir(data_dir) if f.endswith('.npy')]\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    test_data = []\n",
        "\n",
        "    for class_name in class_names:\n",
        "        data = np.load(os.path.join(data_dir, class_name + '.npy'))\n",
        "        labels = np.full(data.shape[0], class_names.index(class_name))\n",
        "        data = data.reshape((-1, 28, 28, 1))  # Reshape the data to (28, 28, 1)\n",
        "        class_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "        class_dataset = class_dataset.shuffle(len(data))\n",
        "\n",
        "        train_size = int(0.7 * len(data))\n",
        "        val_size = int(0.15 * len(data))\n",
        "        test_size = len(data) - train_size - val_size\n",
        "\n",
        "        train_class_data = class_dataset.take(train_size)\n",
        "        val_class_data = class_dataset.skip(train_size).take(val_size)\n",
        "        test_class_data = class_dataset.skip(train_size + val_size).take(test_size)\n",
        "\n",
        "        train_data.append(train_class_data)\n",
        "        val_data.append(val_class_data)\n",
        "        test_data.append(test_class_data)\n",
        "\n",
        "    train_dataset = train_data[0]\n",
        "    val_dataset = val_data[0]\n",
        "    test_dataset = test_data[0]\n",
        "\n",
        "    for train_class_data in train_data[1:]:\n",
        "        train_dataset = train_dataset.concatenate(train_class_data)\n",
        "\n",
        "    for val_class_data in val_data[1:]:\n",
        "        val_dataset = val_dataset.concatenate(val_class_data)\n",
        "\n",
        "    for test_class_data in test_data[1:]:\n",
        "        test_dataset = test_dataset.concatenate(test_class_data)\n",
        "\n",
        "    train_dataset = train_dataset.shuffle(len(train_dataset)).batch(batch_size)\n",
        "    val_dataset = val_dataset.shuffle(len(val_dataset)).batch(batch_size)\n",
        "    test_dataset = test_dataset.shuffle(len(test_dataset)).batch(batch_size)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzrX-yVtAuE"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B21kb9U0tBuj"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_dir = 'Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6UKWi9VtJfL"
      },
      "outputs": [],
      "source": [
        "# Define batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Create training, validation, and test datasets\n",
        "train_dataset, val_dataset, test_dataset, class_names = data_generator(data_dir, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9BMhK-lpo49"
      },
      "source": [
        "# **Build and train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR9UCLLFpuNe"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.3),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.4),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(class_names), activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udeoHVXKtePH"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAj1mOF0v3za"
      },
      "outputs": [],
      "source": [
        "# Reduce learning rate when a metric has stopped improving\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJrjwoVvtmJT"
      },
      "outputs": [],
      "source": [
        "# Number of epochs to train\n",
        "num_epochs = 10  # Change this value to your desired number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZncvATUtnq5",
        "outputId": "e349d6ff-6eed-4a06-9960-7d3794f7f8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "34128/34128 [==============================] - 2100s 61ms/step - loss: 0.3118 - accuracy: 0.9109 - val_loss: 0.2716 - val_accuracy: 0.9228 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "34127/34128 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9266"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, callbacks=[reduce_lr, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo0rYg0u3szn"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test loss: {test_loss}')\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfhpAuN7tsae"
      },
      "source": [
        "# **Save the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHsfo64Jt0Jg"
      },
      "outputs": [],
      "source": [
        "model.save('bestModel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgzNylcLJRwm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('bestModel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX7BQIEG6BbK"
      },
      "outputs": [],
      "source": [
        "all_classes = [\n",
        "        'airplane','banana', 'basketball', 'book',\n",
        "        'car', 'cat', 'fish', 'flower', 'guitar',\n",
        "        'house', 'key', 'laptop', 'lion', 'moon', 'mountain', 'pencil',\n",
        "        'star', 'tree', 'umbrella','sun', 'cloud', 'house', 'lightning'\n",
        "    ]\n",
        "\n",
        "with open(\"class.txt\", \"w\") as file:\n",
        "    for item in all_classes:\n",
        "        file.write(item + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaWIeGqx6DL_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('classes_name.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUon-U7O4Xg7"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Compress the directory into a zip file\n",
        "shutil.make_archive('Dataset', 'zip', 'Dataset')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('Dataset.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
